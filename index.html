<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Infinitusposs - Always trust yourself</title><meta property="og:type" content="blog"><meta property="og:title" content="Infinitusposs - Always trust yourself"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="Infinitusposs - Always trust yourself"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="Yufei Wang"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"Infinitusposs - Always trust yourself","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"Yufei Wang"},"description":""}</script><link rel="icon" href="/img/f_logo.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/blog_logo.png" alt="Infinitusposs - Always trust yourself" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-06-29T13:33:32.000Z" title="2020-06-29T13:33:32.000Z">2020-06-29</time><span class="level-item"><a class="link-muted" href="/categories/Personal-Project/">Personal Project</a></span><span class="level-item">6 minutes read (About 836 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/29/Cross-modal-retrieval-across-audio-and-images/">Cross-modal retrieval across audio and images</a></h1><div class="content"><p>This project aims to retrieve the pertinent sample across images and audio.</p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p><strong>Wikipedia Dataset</strong> <sup>[1]</sup><br>The collected documents are selected sections from the Wikipedia’s featured articles collection. This is a continuously growing dataset, that at the time of collection (October 2009) had 2,669 articles spread over 29 categories. Some of the categories are very scarce, therefore we considered only the 10 most populated ones. The articles generally have multiple sections and pictures. We have split them into sections based on section headings, and assign each image to the section in which it was placed by the author(s). Then this dataset was prunned to keep only sections that contained a single image and at least 70 words.<br>The final corpus contains 2,866 multimedia documents. The median text length is 200 words.<br>You can download Wikipedia Dataset from <a href="http://www.svcl.ucsd.edu/projects/crossmodal" title="Download Wikipedia dataset"><ins><strong>here</strong></ins></a>.</p>
<p><strong>PASCAL sentences Dataset</strong> <sup>[2]</sup><br>This dataset contains 1,000 images from 20 categories, and each image has 5 corresponding sentences as exact descriptions.<br>The original dataset is posted on this <a href="https://vision.cs.uiuc.edu/pascal-sentences/" title="PASCAL sentences dataset"><ins><strong>website</strong></ins></a>.<br>You can download this dataset through <a href="https://github.com/rupy/PascalSentenceDataset" title="Download PASCAL dataset using Python 2"><ins><strong>rupy’s python program</strong></ins></a>.<br>However, rupy’s python program is based on Python 2. The <a href="https://github.com/infinitusposs/PascalSentencePython3" title="Download PASCAL dataset using Python 3"><ins><strong>modified program</strong></ins></a> is based on Python 3, and you can use the program as described in rupy’s program.</p>
<p><strong>IAPR TC-12 Dataset</strong><br>The image collection of the IAPR TC-12 Benchmark consists of 20,000 still natural images taken from locations around the world and comprising an assorted cross-section of still natural images. This includes pictures of different sports and actions, photographs of people, animals, cities, landscapes and many other aspects of contemporary life.<br>Each image is associated with a text caption in up to three different languages (English, German and Spanish) . These annotations are stored in a database which is managed by a benchmark administration system that allows the specification of parameters according to which different subsets of the image collection can be generated.<br>You can download IAPR TC-12 dataset from <a href="https://www.imageclef.org/photodata" title="Download IAPR TC-12 dataset"><ins><strong>here</strong></ins></a>.</p>
<h2 id="Text-To-Speech"><a href="#Text-To-Speech" class="headerlink" title="Text-To-Speech"></a>Text-To-Speech</h2><p>All three datasets only contains images and the corresponding texts. In order to process cross-modal retrieval across images and audio, I need to turn texts into audio. I use <a href="http://www.cross-plus-a.com/balabolka.htm" title="Download Balabolka"><ins><strong>Balabolka</strong></ins></a> for this process. Balabolka is a free TTS program that supports batch processing, which is useful to process huge datasets.</p>
<h2 id="Data-Pre-processing"><a href="#Data-Pre-processing" class="headerlink" title="Data Pre-processing"></a>Data Pre-processing</h2><p>You can download the Python program for pre-processing from <a href="https://github.com/infinitusposs/preprocessing-datasets" title="Download program for preprocessing"><ins><strong>here</strong></ins></a></p>
<p><strong>Wikipedia Dataset</strong><br> I extract the first sentence of every article and save it in TXT file with the same name as the name of the corresponding image.</p>
<p><strong>PASCAL sentences Dataset</strong><br>Texts in PASCAL sentences dataset does not need to be preprocessed since they are already in .txt files. Use Balabolka turn them into audio. </p>
<p><strong>IAPR TC-12 Dataset</strong><br>I only use texts in English for this project. The English texts are in a file extensin of .ENG. The ENG file type is primarily associated with Dictionary, but this type can be treated as an XML file because an ENG file has tags which is the same as an XML file. I use Python 3 to read the ENG file and process them as processing XML files and save every description of the corresponding image in a TXT file with the same name as the name of the corresponding ENG file.</p>
<h2 id="Dataset-after-pre-processing-and-TTS"><a href="#Dataset-after-pre-processing-and-TTS" class="headerlink" title="Dataset after pre-processing and TTS"></a>Dataset after pre-processing and TTS</h2><p>The following datasets contains audio, texts and images. You can download them from Google Drive. </p>
<p><a href="https://drive.google.com/uc?export=download&id=1IqZ0PLMq_pDF31JniwQx5oRdY4gX_Rqw" title="Wikipedia dataset with audio"><ins><strong>Wikipedia Datast</strong></ins></a><br>Text(XML) location: /wikipedia_dataset/texts<br>Text(TXT) location: /wikipedia_datast/audio_text<br>Audio location: /wikipedia_datast/audio<br>Image location: /wikipedia_datast/images</p>
<p><a href="https://drive.google.com/uc?export=download&id=1Z7oL-EFMFS8bRDa0tCANho8GJ2zj1dHV" title="PASCAL sentences dataset with audio"><ins><strong>PASCAL sentences Dataset</strong></ins></a><br>Text location: /PascalSentenceDataset/sentence/(each category)<br>Audio location: /PascalSentenceDataset/sentence/(each category)/(each directory)<br>Image location: /PascalSentenceDataset/dataset</p>
<p><a href="https://drive.google.com/uc?export=download&id=1NA4DQwMMJO4qnA-_Z50XzmW4Lx5YTLx9" title="IAPR TC-12 dataset with audio"><ins><strong>IAPR TC-12 Dataset</strong></ins></a><br>Text location: /iaprtc12/text<br>Audio location: /iaprtc12/audio<br>Image location: /iaprtc12/images</p>
<h2 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h2><p>[1] J. Jeon, V. Lavrenko, and R. Manmatha, “Automatic image annotation<br>and retrieval using cross-media relevance models,” in International<br>ACM SIGIR Conference on Research and Development in Information<br>Retrieval (SIGIR), 2003, pp. 119–126.<br>[2] Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. Collecting Image Annotations Using Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.<br>[3] The IAPR Benchmark: A New Evaluation Resource for Visual Information Systems, Grubinger, Michael, Clough Paul D., Müller Henning, and Deselaers Thomas , International Conference on Language Resources and Evaluation, 24/05/2006, Genoa, Italy, (2006)</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-06-28T12:07:57.000Z" title="2020-06-28T12:07:57.000Z">2020-06-28</time><span class="level-item"><a class="link-muted" href="/categories/Group-Project/">Group Project</a></span><span class="level-item">3 minutes read (About 455 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/28/Market-Forecasting-Using-Deep-Learning-Model/">Market Forecasting Using Deep Learning Model</a></h1><div class="content"><h2 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h2><p>Visit <a href="https://github.com/infinitusposs/CMPT419-Project-by-Triple-A">https://github.com/infinitusposs/CMPT419-Project-by-Triple-A</a></p>
<h2 id="Poster"><a href="#Poster" class="headerlink" title="Poster"></a>Poster</h2><p><img src="/img/poster.jpg" alt="image"></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In recent years, deep learning has developed rapidly. More and more researchers<br>have applied deep learning to different fields, like data analysis, predictions. In<br>terms of predictions, many neural networks, which are an important part of deep<br>learning, have been used in predicting stock price. In this project, we will focus<br>on predicting the revenue of companies. We test and compare the performances<br>of Recurrent Neural Network (RNN) and Multilayer Perceptron (MLP) and try to<br>figure out a better neural network for predicting the revenue of companies. The<br>result indicates that MLP is a better neural network for making predictions on the<br>revenue of companies in the next quarter.</p>
<h2 id="Multi-layer-Perceptron"><a href="#Multi-layer-Perceptron" class="headerlink" title="Multi-layer Perceptron"></a>Multi-layer Perceptron</h2><p><img src="/img/MLP.jpg" alt="image"></p>
<h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p><img src="/img/RNN.png" alt="image"></p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p><a href="https://github.com/infinitusposs/CMPT419-Project-by-Triple-A/blob/master/Dataset/Apple%20Financial%20Report.csv">Apple Financial Report.csv</a></p>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p><img src="/img/MLP_result.png" alt="image"></p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p><img src="/img/RNN_result.png" alt="image"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>RNN performs better on stock market prediction because the price highly depends on the previous days. However, regarding the revenue of companies, the revenue highly depends on the decision they make in the quarter. This is also because we changed the dataset so that whether new products come out has been directly connected to the revenue of the corresponding quarter.<br>In conclusion, Multilayer Perceptron neural network is a better choice for forecasting companies’ profits.</p>
<h2 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h2><p>So far, many neural networks have been used in stock price prediction, such as Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). And for predicting stock prices, CNN is the best choice and both LSTM and RNN have better performances than MLP since stock prices heavily depend on the price of the previous days.<br>For the future work, we are going to test the performances of more neural networks like LSTM. Since LSTM is a special type RNN, we expect that LSTM will have a similar performance as RNN. Also, CNN is another choice for this project. We assume CNN will have the best performance on market profit forecasting since it did well in stock forecasting and it does not depend on the previous data.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Najafabadi, M. M., Villanustre, F., Khoshgoftaar, T. M., Seliya, N., Wald, R., &amp; Muharemagic, E. (2015). Deep learning applications and challenges in big data analytics. Journal of Big Data, 2(1), 1.<br>[2] Vaisla, K. S., &amp; Bhatt, A. K. (2010). An analysis of the performance of artificial neural network technique for stock market forecasting. International Journal on Computer Science and Engineering, 2(6), 2104-2109.<br>[3] Hiransha, M., Gopalakrishnan, E. A., Menon, V. K., &amp; Soman, K. P. (2018). NSE stock market prediction using deep-learning models. Procedia computer science, 132, 1351-1362.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-03-19T10:10:00.000Z" title="2020-03-19T10:10:00.000Z">2020-03-19</time><span class="level-item"><a class="link-muted" href="/categories/Personal-Project/">Personal Project</a></span><span class="level-item">a minute read (About 189 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/19/Writing-using-Air-Gestures/">Writing using Air Gestures</a></h1><div class="content"><h2 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h2><p><a href="https://github.com/infinitusposs/Writing-using-Air-Gestures">https://github.com/infinitusposs/Writing-using-Air-Gestures</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This project is a combination of object detection and image classification based on the IOS edge device. Through the device’s camera, detecting the user’s fingertip and tracking the movement of the fingertip with a black line. And then using image classification to recognize the digits that users have written.</p>
<blockquote>
<p>Instructions: one finger for drawing, two fingers for pausing drawing, three fingers for clearing the screen.</p>
</blockquote>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://docs.google.com/uc?export=download&id=1tG3LsgnMbuZcsekFZQT9Ya6_RaFptNnz" alt="Image description"></h1><h2 id="Video-demo"><a href="#Video-demo" class="headerlink" title="Video demo"></a>Video demo</h2><p><a href="https://drive.google.com/open?id=1USy7P8v0_BMWt6moBg2X78Au7JBwgGZI">Demo</a></p>
<h2 id="Report"><a href="#Report" class="headerlink" title="Report"></a>Report</h2><p>The <a href="https://drive.google.com/open?id=1jyEodGnFKugR1xqLvoQct8RwHDqmLDMX">report</a> contains all the details about the project.</p>
<h2 id="Colab-Notebook"><a href="#Colab-Notebook" class="headerlink" title="Colab Notebook"></a>Colab Notebook</h2><p><a href="https://drive.google.com/open?id=1UzoT5bEjTK-3xBk4jSxnT-T3FgIiluXT">Google Colab</a></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>CPPN: <a href="http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/">http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/</a><br>Google edge TF Lite IOS tutorial: <a href="https://cloud.google.com/vision/automl/object-detection/docs/tflite-ios-tutorial">https://cloud.google.com/vision/automl/object-detection/docs/tflite-ios-tutorial</a><br>Kaggle dataset “Fingers”: <a href="https://www.kaggle.com/koryakinp/fingers">https://www.kaggle.com/koryakinp/fingers</a> (Unused for the final version)</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/hayate.jpeg" alt="Infinitusposs"></figure><p class="title is-size-4 is-block line-height-inherit">Infinitusposs</p><p class="is-size-6 is-block">SFU Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">13</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/infinitusposs" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/infinitusposs/"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://fantasyonly.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Brother Nie</span></span><span class="level-right"><span class="level-item tag">fantasyonly.github.io</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/03/"><span class="level-start"><span class="level-item">March 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Group-Project/"><span class="level-start"><span class="level-item">Group Project</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Personal-Project/"><span class="level-start"><span class="level-item">Personal Project</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-06-29T13:33:32.000Z">2020-06-29</time></p><p class="title is-6"><a class="link-muted" href="/2020/06/29/Cross-modal-retrieval-across-audio-and-images/">Cross-modal retrieval across audio and images</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Personal-Project/">Personal Project</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-06-28T12:07:57.000Z">2020-06-28</time></p><p class="title is-6"><a class="link-muted" href="/2020/06/28/Market-Forecasting-Using-Deep-Learning-Model/">Market Forecasting Using Deep Learning Model</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Group-Project/">Group Project</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-19T10:10:00.000Z">2020-03-19</time></p><p class="title is-6"><a class="link-muted" href="/2020/03/19/Writing-using-Air-Gestures/">Writing using Air Gestures</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Personal-Project/">Personal Project</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Computer-Vision/"><span class="tag">Computer Vision</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-modal-retrieval/"><span class="tag">Cross-modal retrieval</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-learning/"><span class="tag">Deep learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image-classification/"><span class="tag">Image classification</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLP/"><span class="tag">MLP</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Market-Forecasting/"><span class="tag">Market Forecasting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Object-detection/"><span class="tag">Object detection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python-3/"><span class="tag">Python 3</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Swift/"><span class="tag">Swift</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wikipedia-dataset/"><span class="tag">Wikipedia dataset</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/iOS/"><span class="tag">iOS</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/blog_logo.png" alt="Infinitusposs - Always trust yourself" height="28"></a><p class="size-small"><span>&copy; 2020 Yufei Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://yoursite.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>